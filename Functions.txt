This repository also contains important activation functions used in machine learning!!

1.) Sigmoid Function - Used in binary classification.The main reason why we use sigmoid function is because it exists between (0 to 1). 
                       Therefore, it is especially used for models where we have to predict the probability as an output.
                       Since probability of anything exists only between the range of 0 and 1, sigmoid is the right choice.
                       
____________________________________________________________________________________________________________________________________________________________________________________

2.) TanH or Hyperbolic Tangent - The advantage is that the negative inputs will be mapped strongly negative and the zero inputs will be mapped near zero in the tanh graph.
                    The function is differentiable.
                    The function is monotonic while its derivative is not monotonic.
                    The tanh function is mainly used classification between two classes.
                    Both tanh and logistic sigmoid activation functions are used in feed-forward nets.

_____________________________________________________________________________________________________________________________________________________________________________________

3.) Softmax Activation Function - The softmax function is often used in the final layer of a neural network-based classifier. 
                                  Such networks are commonly trained under a log loss (or cross-entropy) regime, giving a non-linear 
                                  variant of multinomial logistic regression.
                                  
 ____________________________________________________________________________________________________________________________________________________________________________________
